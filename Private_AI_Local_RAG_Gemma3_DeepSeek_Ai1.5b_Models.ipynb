{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "890a418d-d5cc-48d1-aef5-e04c6e37c114",
   "metadata": {},
   "source": [
    "# **INM's Private AI - RAG Streamlit App**\n",
    "\n",
    "This notebook provides a detailed, step-by-step explanation of a Retrieval-Augmented Generation (RAG) system built using Streamlit. The system:\n",
    "\n",
    "- **Supports multiple document uploads** (PDF, DOCX, PPTX)\n",
    "- **Extracts and cleans text** using libraries such as *pdfplumber*, *python-docx*, and *python-pptx*\n",
    "- **Embeds text chunks** using a SentenceTransformer model\n",
    "- **Performs retrieval** using FAISS\n",
    "- **Generates answers** using a Hugging Face language model (with GPU support if available)\n",
    "- **Post-processes** the model’s output to remove any `<think>...</think>` tags so that only the final answer is shown\n",
    "\n",
    "The code is organized into sections with detailed explanations for each function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111beb30-7f98-4683-bb9a-53b2b4002bf9",
   "metadata": {},
   "source": [
    "### Cell 2: Importing Required Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31100a1-e26e-4f9d-80c7-3906c006291f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Standard and Numerical Libraries**\n",
    "import os              # For file and path operations.\n",
    "import re              # For regex-based text cleaning.\n",
    "import tempfile        # For creating temporary files during document uploads.\n",
    "import numpy as np     # For numerical operations, especially for handling embeddings.\n",
    "\n",
    "# **Deep Learning and Device Management**\n",
    "import torch           # PyTorch for model inference and device (GPU/CPU) management.\n",
    "\n",
    "# **Streamlit for UI**\n",
    "import streamlit as st # For building the web UI of the RAG system.\n",
    "\n",
    "# **Document Processing Libraries**\n",
    "import pdfplumber      # For advanced PDF text extraction.\n",
    "from docx import Document  # For reading DOCX files.\n",
    "from pptx import Presentation # For reading PPTX files.\n",
    "\n",
    "# **Hugging Face Libraries**\n",
    "from transformers import AutoConfig, AutoTokenizer, AutoModelForCausalLM  \n",
    "# AutoConfig loads model configurations; AutoTokenizer tokenizes text; AutoModelForCausalLM is for language generation.\n",
    "\n",
    "# **Sentence Transformers**\n",
    "from sentence_transformers import SentenceTransformer  \n",
    "# To generate dense vector embeddings from text.\n",
    "\n",
    "# **FAISS for Similarity Search**\n",
    "import faiss           # For efficient vector similarity search (runs on CPU).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b1c329-b410-4ac8-8ba8-14c98c86ab31",
   "metadata": {},
   "source": [
    "# Explanation:\n",
    "\n",
    "Purpose: We import all the necessary libraries to build our RAG system. \n",
    "Key Points: \n",
    "os, re, tempfile, numpy: Standard modules for file handling, regular expressions, temporary file management, and numerical operations. \n",
    "torch: For deep learning, handling models, and using GPUs. \n",
    "streamlit: Provides the web interface.  \n",
    "pdfplumber, python-docx, python-pptx: To extract text from various document types.  \n",
    "transformers and SentenceTransformer: To load and run our language and embedding models.  \n",
    "faiss: For quickly retrieving similar text chunks.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546611b9-e56f-4772-83e1-94a686de5a00",
   "metadata": {},
   "source": [
    "# Cell 3: Streamlit Configuration and Device Setupt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a9bd24-4c3b-4e32-9a76-33420c04193b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Streamlit Configuration:**\n",
    "# Set up the Streamlit app's title and layout. This must be the first Streamlit call.\n",
    "st.set_page_config(page_title=\"INM's Private AI\", layout=\"wide\")\n",
    "\n",
    "# **Device Setup:**\n",
    "# Determine whether to use GPU (\"cuda\") or CPU (\"cpu\") based on availability.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "st.write(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb1ac89-4448-440b-ad6c-2926a25da55f",
   "metadata": {},
   "source": [
    "## Explanation:\n",
    "\n",
    "st.set_page_config: Configures the app's title and layout.  \n",
    "Device Check: We check if a GPU is available using torch.cuda.is_available(). This is critical for performance with large models.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724eb87f-ce4c-451e-8b74-7ea3c1f771c2",
   "metadata": {},
   "source": [
    "# Cell 4: Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819688e6-317e-479a-afed-6086289b44ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Model Selection Sidebar:**\n",
    "st.sidebar.header(\"Model Selection\")\n",
    "model_option = st.sidebar.selectbox(\n",
    "    \"Select the model to use\", \n",
    "    options=[\"DeepSeek-R1-Distill-Qwen-1.5B\", \"Gemma-3-1b-it\"]\n",
    ")\n",
    "\n",
    "# **Setting Model Paths and Configurations:**\n",
    "# Use placeholders for your directories. Replace <MODEL_PATH_DEEPSEEK> and <MODEL_PATH_GEMMA> with actual paths.\n",
    "if model_option == \"DeepSeek-R1-Distill-Qwen-1.5B\":\n",
    "    MODEL_PATH = r\"<MODEL_PATH_DEEPSEEK>\"  # e.g., \"C:\\Path\\To\\DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "    disable_swa = True  # Disable sliding window attention for DeepSeek-R1.\n",
    "else:\n",
    "    MODEL_PATH = r\"<MODEL_PATH_GEMMA>\"       # e.g., \"C:\\Path\\To\\gemma-3-1b-it\"\n",
    "    disable_swa = False\n",
    "\n",
    "# **Embedding Model:**\n",
    "# We use the same embedding model for both options.\n",
    "EMBEDDING_MODEL = \"sentence-transformers/all-mpnet-base-v2\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db4a565-ae9b-4fe1-a98e-3254cedd7633",
   "metadata": {},
   "source": [
    "## Explanation:\n",
    "\n",
    "Model Selection: The sidebar allows selection between two models.  \n",
    "Configuration: Depending on the selection, a model path is set and a flag (disable_swa) is used to modify the configuration.  \n",
    "Placeholders: Personal directories are replaced by placeholders.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a3631a-8ff8-4e6e-b25e-cfc56b52f796",
   "metadata": {},
   "source": [
    "# Cell 5: Text Cleaning Helpers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5746ad-e0b5-4b71-9b7e-fc8e8e05e397",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collapse_repeated_chars(text: str, threshold=3) -> str:\n",
    "    \"\"\"\n",
    "    **Purpose:** Collapse runs of a character that repeat 'threshold' or more times.\n",
    "    \n",
    "    **Example:** \"aaaaa\" becomes \"aa\" if threshold=3.\n",
    "    \"\"\"\n",
    "    pattern = rf\"(.)\\1{{{threshold-1},}}\"\n",
    "    return re.sub(pattern, r\"\\1\\1\", text)\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    **Purpose:** Clean raw text by:\n",
    "      - Removing unwanted lines (e.g., headers like \"PAGE\" or copyright symbols).\n",
    "      - Collapsing repeated characters.\n",
    "    \n",
    "    **Explanation:** Splits the text into lines, filters out those containing \"PAGE\" or \"©\", then joins and collapses repeated characters.\n",
    "    \"\"\"\n",
    "    lines = text.split(\"\\n\")\n",
    "    cleaned_lines = []\n",
    "    for line in lines:\n",
    "        if \"PAGE\" in line.upper():\n",
    "            continue\n",
    "        if \"©\" in line:\n",
    "            continue\n",
    "        cleaned_lines.append(line)\n",
    "    joined = \"\\n\".join(cleaned_lines)\n",
    "    return collapse_repeated_chars(joined)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adba34e5-0c38-42a4-88c1-def1a5641d91",
   "metadata": {},
   "source": [
    "# Explanation:\n",
    "\n",
    "collapse_repeated_chars: Uses regular expressions to simplify sequences of repeated characters.     \n",
    "clean_text: Filters out lines that are likely not useful and then applies the collapse function.     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30852ac8-0bd5-4ef0-8bdd-f9e620095eb8",
   "metadata": {},
   "source": [
    "# Cell 6: Post-Processing Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68699370-cfbb-476a-a098-d5d75452eca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process_response(raw_answer: str) -> str:\n",
    "    \"\"\"\n",
    "    **Purpose:** Clean the model's raw output by:\n",
    "      1. Removing any text enclosed in `<think>...</think>` tags.\n",
    "      2. Removing the \"Final Answer:\" prefix if it appears.\n",
    "    \n",
    "    **Explanation:** The function uses regex to detect and remove these tags, ensuring the final output contains only the answer.\n",
    "    \"\"\"\n",
    "    # Remove any chain-of-thought markers within <think>...</think>\n",
    "    processed = re.sub(r\"<think>.*?</think>\", \"\", raw_answer, flags=re.DOTALL).strip()\n",
    "    # Remove the \"Final Answer:\" prefix (case-insensitive)\n",
    "    processed = re.sub(r\"(?i)^final answer:\\s*\", \"\", processed)\n",
    "    return processed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92f9fd9-afd1-4bef-99a3-c90ec3ed08a5",
   "metadata": {},
   "source": [
    "## Explanation:\n",
    "\n",
    "post_process_response: Strips out internal reasoning markers and any leading \"Final Answer:\" text so that only the final answer remains. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338f44c5-a804-4552-8fce-c1c7588d474b",
   "metadata": {},
   "source": [
    "# Cell 7: RAG System Class Definition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c5f253-5d6d-47ce-9d16-4c722c764d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentRAG:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        **Initialization:**\n",
    "        - Loads model configuration from the chosen model path.\n",
    "        - Disables sliding window attention if specified.\n",
    "        - Loads the tokenizer, language model (LLM) with half-precision for efficiency, and the SentenceTransformer embedding model.\n",
    "        - Initializes a FAISS index for retrieval.\n",
    "        - Sets up counters and chunking parameters.\n",
    "        \"\"\"\n",
    "        config = AutoConfig.from_pretrained(MODEL_PATH)\n",
    "        if disable_swa:\n",
    "            config.use_sliding_window_attention = False\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "        self.llm = AutoModelForCausalLM.from_pretrained(\n",
    "            MODEL_PATH,\n",
    "            config=config,\n",
    "            torch_dtype=torch.float16\n",
    "        ).to(device)\n",
    "\n",
    "        self.embedder = SentenceTransformer(EMBEDDING_MODEL, device=device)\n",
    "\n",
    "        dim = self.embedder.get_sentence_embedding_dimension()\n",
    "        self.index = faiss.IndexFlatL2(dim)\n",
    "        self.doc_store = []\n",
    "\n",
    "        self.input_tokens = 0\n",
    "        self.output_tokens = 0\n",
    "\n",
    "        self.chunk_size = 1000  # Length of each text chunk\n",
    "        self.overlap = 100      # Overlap between consecutive chunks\n",
    "\n",
    "    def _extract_text_pdf(self, file_path: str) -> str:\n",
    "        \"\"\"\n",
    "        **Purpose:** Extracts text from a PDF using pdfplumber.\n",
    "        **Explanation:** Iterates through each page and collects the text.\n",
    "        \"\"\"\n",
    "        text = \"\"\n",
    "        with pdfplumber.open(file_path) as pdf:\n",
    "            for page in pdf.pages:\n",
    "                page_text = page.extract_text() or \"\"\n",
    "                text += page_text + \"\\n\"\n",
    "        return text\n",
    "\n",
    "    def _extract_text_docx(self, file_path: str) -> str:\n",
    "        \"\"\"\n",
    "        **Purpose:** Extracts text from a DOCX file.\n",
    "        **Explanation:** Reads all paragraphs and joins their text.\n",
    "        \"\"\"\n",
    "        doc = Document(file_path)\n",
    "        return \"\\n\".join(p.text for p in doc.paragraphs)\n",
    "\n",
    "    def _extract_text_pptx(self, file_path: str) -> str:\n",
    "        \"\"\"\n",
    "        **Purpose:** Extracts text from a PPTX file.\n",
    "        **Explanation:** Iterates over slides and shapes to collect text.\n",
    "        \"\"\"\n",
    "        prs = Presentation(file_path)\n",
    "        text_runs = []\n",
    "        for slide in prs.slides:\n",
    "            for shape in slide.shapes:\n",
    "                if hasattr(shape, \"text\"):\n",
    "                    text_runs.append(shape.text)\n",
    "        return \"\\n\".join(text_runs)\n",
    "\n",
    "    def _extract_text(self, file_path: str, extension: str) -> str:\n",
    "        \"\"\"\n",
    "        **Purpose:** Determines the file type (PDF, DOCX, PPTX) and extracts text accordingly.\n",
    "        **Explanation:** Calls the appropriate extraction function and then cleans the text.\n",
    "        \"\"\"\n",
    "        extension = extension.lower()\n",
    "        if extension.endswith(\".pdf\"):\n",
    "            raw_text = self._extract_text_pdf(file_path)\n",
    "        elif extension.endswith(\".docx\"):\n",
    "            raw_text = self._extract_text_docx(file_path)\n",
    "        elif extension.endswith(\".pptx\"):\n",
    "            raw_text = self._extract_text_pptx(file_path)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported file format\")\n",
    "        return clean_text(raw_text)\n",
    "\n",
    "    def _chunk_text(self, text: str) -> list:\n",
    "        \"\"\"\n",
    "        **Purpose:** Splits the cleaned text into overlapping chunks.\n",
    "        **Explanation:** This is useful to fit the text into the model's context window.\n",
    "        \"\"\"\n",
    "        chunks = []\n",
    "        stride = self.chunk_size - self.overlap\n",
    "        for i in range(0, len(text), stride):\n",
    "            chunks.append(text[i : i + self.chunk_size])\n",
    "        return chunks\n",
    "\n",
    "    def ingest_document(self, file_path: str, original_filename: str):\n",
    "        \"\"\"\n",
    "        **Purpose:** Ingests a document by:\n",
    "          - Extracting text based on file type.\n",
    "          - Cleaning and chunking the text.\n",
    "          - Generating embeddings for each chunk.\n",
    "          - Storing the embeddings in the FAISS index and the raw chunks in the document store.\n",
    "        \"\"\"\n",
    "        ext = os.path.splitext(original_filename)[1]\n",
    "        text = self._extract_text(file_path, ext)\n",
    "        chunks = self._chunk_text(text)\n",
    "        embeddings = self.embedder.encode(chunks, convert_to_tensor=True)\n",
    "        embeddings = embeddings.cpu().numpy().astype(np.float32)\n",
    "        self.index.add(embeddings)\n",
    "        self.doc_store.extend(chunks)\n",
    "        self.input_tokens += sum(len(self.tokenizer.encode(chunk)) for chunk in chunks)\n",
    "\n",
    "    def query(self, question: str, top_k: int = 3, max_length: int = 512) -> str:\n",
    "        \"\"\"\n",
    "        **Purpose:** Answers a user query by:\n",
    "          - Encoding the query.\n",
    "          - Retrieving the top_k most similar text chunks from the FAISS index.\n",
    "          - Building a prompt with the retrieved context.\n",
    "          - Generating a response using the language model.\n",
    "          - Post-processing the response to remove any unwanted markers.\n",
    "        \"\"\"\n",
    "        if not self.doc_store:\n",
    "            return \"No documents have been uploaded yet. Please upload a file first.\"\n",
    "\n",
    "        query_tensor = self.embedder.encode([question], convert_to_tensor=True)\n",
    "        query_vec = query_tensor.cpu().numpy().astype(np.float32)\n",
    "        distances, indices = self.index.search(query_vec, top_k)\n",
    "        retrieved_chunks = [self.doc_store[i] for i in indices[0] if i < len(self.doc_store)]\n",
    "        context = \"\\n\".join(retrieved_chunks)\n",
    "\n",
    "        # Build the prompt. Note that we include context and the question.\n",
    "        prompt = (\n",
    "            \"You are a helpful assistant. Based on the context provided, give a concise final answer. \"\n",
    "            \"Do not include your chain-of-thought or internal reasoning in the output.\\n\\n\"\n",
    "            f\"Context:\\n{context}\\n\\n\"\n",
    "            f\"Question: {question}\\n\\n\"\n",
    "            \"Final Answer:\"\n",
    "        )\n",
    "\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        outputs = self.llm.generate(**inputs, max_length=max_length)\n",
    "        raw_answer = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        final_answer = post_process_response(raw_answer)\n",
    "        self.input_tokens += len(inputs[\"input_ids\"][0])\n",
    "        self.output_tokens += len(outputs[0])\n",
    "        return final_answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179e0194-caf3-40df-b9d4-562501610c18",
   "metadata": {},
   "source": [
    "## Explanation:\n",
    "\n",
    "**Initialization:**\n",
    "The __init__ method loads the chosen language model, tokenizer, and embedding model onto the appropriate device (GPU if available). It also initializes a FAISS index and sets chunking parameters.\n",
    "\n",
    "**Text Extraction Functions:**\n",
    "These functions extract text from PDFs, DOCX, and PPTX files. They are then cleaned using our clean_text helper.\n",
    "\n",
    "**Chunking:**\n",
    "The _chunk_text method splits the cleaned text into overlapping chunks so that it fits within the model's context window.\n",
    "\n",
    "**Ingestion:**\n",
    "The ingest_document method extracts, cleans, chunks, embeds, and stores document text.\n",
    "\n",
    "**Query:**\n",
    "The query method retrieves relevant text chunks using FAISS, constructs a prompt, and uses the language model to generate an answer. The answer is post-processed to remove <think> tags and any leading \"Final Answer:\" text.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0881a2a-f50c-4b48-a4b6-a992ea725de4",
   "metadata": {},
   "source": [
    "# Cell 8: Streamlit UI\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6108627-7224-4273-a733-759f18bfce78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Initializing the RAG System in Session State:**\n",
    "if \"rag\" not in st.session_state:\n",
    "    with st.spinner(f\"Loading {model_option} model...\"):\n",
    "        try:\n",
    "            st.session_state.rag = DocumentRAG()\n",
    "        except Exception as e:\n",
    "            st.error(f\"Failed to initialize model: {str(e)}\")\n",
    "            st.stop()\n",
    "\n",
    "# **Sidebar - Document Upload:**\n",
    "with st.sidebar:\n",
    "    st.header(\"Upload Your Documents\")\n",
    "    uploaded_files = st.file_uploader(\n",
    "        \"Upload PDFs, DOCX, or PPTX files\",\n",
    "        type=[\"pdf\", \"docx\", \"pptx\"],\n",
    "        accept_multiple_files=True\n",
    "    )\n",
    "    if uploaded_files:\n",
    "        for file in uploaded_files:\n",
    "            ext = os.path.splitext(file.name)[1]\n",
    "            # Create a temporary file with the original extension\n",
    "            with tempfile.NamedTemporaryFile(suffix=ext, delete=False) as tmp:\n",
    "                tmp.write(file.getvalue())\n",
    "                st.session_state.rag.ingest_document(tmp.name, file.name)\n",
    "            os.unlink(tmp.name)\n",
    "        st.success(f\"Processed {len(uploaded_files)} file(s)\")\n",
    "    st.markdown(\"---\")\n",
    "    st.markdown(f\"**Input Tokens:** `{st.session_state.rag.input_tokens}`\")\n",
    "    st.markdown(f\"**Output Tokens:** `{st.session_state.rag.output_tokens}`\")\n",
    "\n",
    "# **Main Chat Interface:**\n",
    "st.title(\"🔎 INM's Private AI\")\n",
    "\n",
    "if \"messages\" not in st.session_state:\n",
    "    st.session_state.messages = []\n",
    "\n",
    "# **Display Chat History:**\n",
    "for message in st.session_state.messages:\n",
    "    with st.chat_message(message[\"role\"]):\n",
    "        st.markdown(message[\"content\"])\n",
    "\n",
    "# **Chat Input and Response:**\n",
    "if prompt := st.chat_input(\"Ask a question about your documents...\"):\n",
    "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "    with st.chat_message(\"user\"):\n",
    "        st.markdown(prompt)\n",
    "    with st.spinner(\"Thinking...\"):\n",
    "        try:\n",
    "            response = st.session_state.rag.query(prompt, max_length=1024)\n",
    "        except Exception as e:\n",
    "            response = f\"Error: {str(e)}\"\n",
    "    with st.chat_message(\"assistant\"):\n",
    "        st.markdown(response)\n",
    "    st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a1fb1e-3dcd-4e99-8524-76e355f22fa5",
   "metadata": {},
   "source": [
    "## Explanation:\n",
    "\n",
    "**Session State Initialization:**\n",
    "We initialize our DocumentRAG instance and store it in Streamlit’s session state to avoid reloading on every interaction.\n",
    "\n",
    "**File Upload Section (Sidebar):**\n",
    "Users can upload multiple documents. Each document is temporarily saved (with its original extension preserved) and processed.\n",
    "\n",
    "**Chat Interface:**\n",
    "Displays past messages and provides an input field for new queries. When a query is submitted, it is processed, and the response is displayed.\n",
    "\n",
    "**Token Counters:**\n",
    "The sidebar shows input and output token counts (useful for monitoring model usage).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bea6d55-1d8a-472c-b8b7-09333fa2bc50",
   "metadata": {},
   "source": [
    "# Cell 9: How to Run the App (Markdown)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72801760-14cd-423e-8ef5-ff85d521e646",
   "metadata": {},
   "source": [
    "## **How to Run the App**\n",
    "\n",
    "1. **Save this notebook as a Python script** (e.g., `rag_app.py`) or export it as one.\n",
    "2. **Open your Command Prompt or Anaconda Prompt** and navigate to the directory containing your script.  \n",
    "   Example:\n",
    "   ```bash\n",
    "   cd <YOUR_PROJECT_DIRECTORY>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559e5e56-01fb-40ee-a8df-807580455d93",
   "metadata": {},
   "source": [
    "3. Run the Streamlit app using:\n",
    "\n",
    "  ** python -m streamlit run rag_app.py **\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85281d63-7bff-4113-b4d3-cd8eae91538d",
   "metadata": {},
   "source": [
    "Access the app in your browser (usually at http://localhost:8501)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1117c8-f711-401f-960d-eca637df242d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
